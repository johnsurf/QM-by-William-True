\section{Mathematical Framework of Quantum Mechanics}

Much of what I discuss here will be found in Chapter VII of Messiah. He prbably does the most
complete and detailed treatment of the pertinent mathematics which relates to quantum 
mechanics. He is, in fact, more detailed that I will be here and I urge you to read this
Chapter.

Most beginning graduate students, and most likely you are no exceptions, have worked with
quantum mechanics in the ``coordinate represenation'' or the "Schr\"ofinger Picture'' as it is commonly called. But one could also work in the ``momentum representation'' where $\vec{p}$ is replaced by the operator $\vec p$ and $\vec x$ replaced by the operator 
$-{\hbar\over i}\nabla_{\vec p}$. In fact, if you write down the differential equation to be 
solved in the momentum representation for the harmonic oscillator, you will find that you have the same differential operator to solve as you did in the ``coordinate represenation.'' Of course, constant coefficients may be changed in the differential equation in the two representations. Then one can ask the question: -- Are there other representations which one could work in? The answer to this is ``Yes''! In fact, there are in infinite number of them. 
In fact, one can solve the wave equation without ever going into a representation.--A little later, I will show this explicitly where I will solve the harmonic oscillator without going into \underline{any} representation. Often times in more complicated systems, it ``complicates things'' to be in a specific representation.

Now we will want to formulate Quantum Mechanics in a more abstract way so that it is independent of the representation. 
We will then show that if we go into the coordintate representation, it will reduce to out ``familiar'' 
Schr\"odinger picture.  Let us now define a field.\\

\underline{Definition of a Field}

A set of scalars $\{\alpha,\beta,\gamma,...\}$ form a field, F, if the scalars have the following properties:\\

A. To every pair, $\alpha$ and $\beta$, there corresponds a scalar in the field, $\alpha + \beta$, called the sum of $\alpha$ and $\beta$ in such a way that:

1. $\alpha + \beta = \beta + \alpha$ \quad\quad 		--- Communicative\\
2. $\alpha + (\beta + \gamma) = (\alpha + \beta) + \gamma $ 	--- Associative\\
3. $\alpha + 0 = \alpha $					--- Null Scalar Exists\\
4. $\alpha + (-\alpha) = 0$					--- Inverse Exists\\

and B. To every pair, $\alpha$ and $\beta$, there corresponds a scalar, $\alpha\beta$, in such a way that 

1. $\alpha\beta = \beta\alpha$					--- Communicative\\
2. $\alpha(\beta\gamma) = (\alpha\beta)\gamma$			--- Associative\\
3. $\alpha I = \alpha$ 						--- A unique unit scalar exists\\
4. $\alpha \alpha^{-1} = I \rm ~ for \alpha \ne 0$		--- An inverse exists\\

and C. 

$\alpha(\beta + \gamma) = \alpha\beta + \alpha\gamma$		--- Multiplication is distributive with respect to 
addition.\\

Some examples of a field under regular addition and multiplication:

1. The set of all real rational numbers, $\{Q\}$.\\
2. The set of all real numbers, $\{R\}$. \\
3. The set of all complex numbers, $\{C\}$. \\

We now define a linear vector space.\\

\underline{Definition of a Linear Vector Space}

Consider the scalars, $\alpha$, $\beta$, $\gamma$,..., of the field F. Also consider a set of elements. $\ket{x}$, $
\ket{y}$, $\ket{z}$,... called vectors. This set of vectors form a \underline{linear} \underline{vector} \underline{space},
V, when they satisfy: \\

A.) To every pair, $\ket{x}$ and $\ket{y}$, there corresponds a vector $\ket{x} + \ket{y}$ in V 
called the sum of $\ket{x}$
and $\ket{y}$ in such a way that:\\

1. $\ket{x} + \ket{y} = \ket{y} + \ket{x}$ 		--- Communicative\\
2. $\ket{x} + (\ket{y} + \ket{z}) = (\ket{x} + \ket{y}) + \ket{z}$ --- Associative\\
3. $0 = \ket{0}$ and $0 + \ket{x} = \ket{x} + 0 = \ket{x}$	--- A null vector exists\\
4. $\ket{x} + (-\ket{x}) = 0$			--- an inverse exitst.\\

B.) To every pair $\alpha$ and $\beta$ in F and $\ket{x}$ in V, there corresponds a vector $\alpha\ket{x}$ in V, 
called the product of $\alpha$ and $\ket{x}$, such that:\\

1. $\alpha(\beta\ket{x}) = (\alpha\beta)\ket{x}$ --- Associative\\
2. $I\ket{x} = \ket{x}$\\ 

C.) 1. $\alpha(\ket{x} + \ket{y}) = \alpha\ket{x} +\alpha\ket{y} $ --- Multiplication by a scalar 
is distributive with respect to vector addition\\

2. $(\alpha + \beta)\ket{x} = \alpha\ket{x} + \beta\ket{y}$ --- Multiplication by a vector is distributive with 
respect to scalar addition. \\

We continue on with more mathematical definitions and theorems (which I won't always prove).\\

\underline{Definition of Span}

The set of vectors, $\{\ket{x_1},\ket{x_2},...,\ket{x_n}\}$ are said to Span, or Generate, the space V if any vector,
$\ket{x} \in V$ is expressible as a linear combination of them, i.e., if $\ket{x} = \lambda_1\ket{x_1} + 
\lambda_2\ket{x_2} +...+ \lambda_n\ket{x_n}$ for some scalars $\lambda_1, \lambda_2,...,\lambda_n \in F$, which are
not necessarily unique.\\

\underline{Definition of Linearly Independent}

In the abstract vector space V the finite set of vectors $\{\ket{x_1},\ket{x_2},...,\ket{x_n}\}$ is said to be 
linearly dependent if scalars $\lambda_1, \lambda_2,...,\lambda_n \in F$ exist, not all zero, such that
$\ket{x} = \lambda_1\ket{x_1} + \lambda_2\ket{x_2} +...+ \lambda_n\ket{x_n} = \ket{0}$. If no such scalars exist, 
i.e., if $\ket{x} = \lambda_1\ket{x_1} + \lambda_2\ket{x_2} +...+ \lambda_n\ket{x_n} = \ket{0} \Rightarrow 
\lambda_i = 0 \forall i = 1, ..., n$ then the set is linearly independent.\\

\underline{Definition of a Basis}\\

The set $S = \{\ket{x_1},\ket{x_2},...,\ket{x_n}\}$ is a basis of V if (i) they are linearly 
independent and (ii) they span V. If V possesses a (finite) basis it is said to be finite dimensional, 
if not then it is infinite dimensional.

\newtheorem{thma}{Theorem}
\hskip 0.5truein
\hbox{\vbox{
\begin{thma} If $S= \{\ket{x_1},\ket{x_2},...,\ket{x_n}\}$ span V then they form a basis\\
             if and only if any vector $\ket{x}$ in V is uniquely expressible as a linear \\
             combination of the elements of S.
\end{thma}}}

\hskip 0.5truein
\hbox{\vbox{
\begin{thma} If $S= \{\ket{x_1},\ket{x_2},...,\ket{x_n}\}$ span V then there \\
             is a subset of these which is a basis of V.
\end{thma}}}

\hskip 0.5truein
\hbox{\vbox{
\begin{thma} If $S= \{\ket{x_1},\ket{x_2},...,\ket{x_p}\}$ are linearly independent \\
             and V is finite dimensional then there exists a base containing \\
            $\{\ket{x_1},\ket{x_2},...,\ket{x_p}\}.$ 
\end{thma}}}

\hskip 0.5truein
\hbox{\vbox{
\begin{thma} If $\{\ket{x_1},\ket{x_2},...,\ket{x_p}\}$ is a linearly independent set\\
             and $\{\ket{y_1},\ket{y_2},...,\ket{y_m}\}$ spans V, then $p \le m$,
\end{thma}}}


\hskip 0.5truein
\hbox{\vbox{
\begin{thma} The number of elements in any basis of a finite-dimensional \\
             vector space is the same as in any other basis.
\end{thma}}}

The proof of this case can go as follows:\\
Consider two sets of vectors, $S_1 = \{\ket{x_1},\ket{x_2},...,\ket{x_n}\}$ and 
$S_2 = \{\ket{y_1},\ket{y_2},...,\ket{y_m}\}$ where $S_1$ spans the space but all the elements
in $S_1$ \underline{may or may not} be linearly independent, and all the elements in $S_2$ are
linearly independent but $S_2$ \underline{may of may not} span the space. 

If $S_2$ does not span the space, we can find an element in $S_1$, callit $\ket{x_1'}$, which is linearly independent
of the $\ket{y_i}$'s. If $S_2' = \{\ket{y_1},\ket{y_2},...,\ket{y_m},\ket{x'}\}$ does not span the space, we can 
find another linearly independent vector $\ket{x_2'}$ from $S_1$ and add it to $S_2'$ and so on and so forth until
$S_2^p = \{\ket{y_1},\ket{y_2},...,\ket{y_m},\ket{x'},...,\ket{x_p'}\}$ spans the space and forms a basis. Clearly
$p < n$ and $m \le n$. Reversing the rols of $S_2$ and $S_2$, we would find $m \ge n$ and so $m = n$ when $S_1$ and
$S_2$ are both bases.\\

\underline{Definition of Dimensions}\\
The dimension of a finite dimensional vector space is the number of elements in a basis.\\

\hskip 0.5truein
\hbox{\vbox{
\begin{thma} Every $n+1$ vectors in an n-dimensional \\
             vector space is are linearly dependent.
\end{thma}}}

\underline{Definition of Isomorphism}
Two vector spaces $U$ and $V$ over the same field F are said to be isomorphic to each other if there is a one-to-one
correspondence between the vectors $\ket{x} \in U$ and the vector $\ket{y} \in V$, say $\ket{y} = T(\ket{x})$, such
that $$T(\alpha_1\ket{x_1} + \alpha_2\ket{x_2}) = \alpha_1T(\ket{x_1}) + \alpha_2T(\ket{x_2}).$$

That is, all linear relationships are preserved. 

\hskip 0.5truein
\hbox{\vbox{
\begin{thma} Any finite dimensional vector space V is isomorphic to the space of \\
             n-dimensional co-ordinate vectors, the co-ordinates being members of \\
             the scalar field $F$ and $n$ being the dimension of V. 
\end{thma}}}

\underline{Definition of Unitary Space}\\

A ``Unitary Space'' is a linear vector space such that for any two vectors, $\ket{x}$ and $\ket{y}$, a unique scalar
-- called the ``inner product'' or ``scalar product'' and denoted $\braket{x}{y}$ exists and has the following
4 properties:\\

1. $\braket{x}{y} = \braket{y}{x}*$\\
2. $\braket{x}{y + z} = \braket{x}{y} + \braket{x}{z}$\\
3. $\braket{x}{\alpha y} = \alpha\braket{x}{y}$\\
4. $\braket{x}{x} \ge 0$ where $\braket{x}{x} = 0\iff \ket{x} = 0.$\\

This scalar product is essentially the same as the one we introduced in Section I except it ``applies'' to vectors
instead of scalar functions. 

When dealing with finite dimensional unitary spaces, one doesn't have too much trouble. However, for infinite
dimensional unitary spaces which we often have to deal with in quantum mechanics, we can have ``infinities''
appearing unless we place a further restrictions on this space. Consequently, we will only consider Unitary Spaces
which are \underline{complete}.\\ 

\underline{Definition of Hilbert Space}
A complete Unitary Space is called a Hilbert Space. Equivalently, a complete
linear vector space of finite of infinite dimension with scalar product is a Hilbert Space.

By being complete, we mean that for a sequence of vectors, $\ket{x_n}$, a vector $\ket{x}$ exists such that
$$\lim_{n\rightarrow \infty} \| x_n - x \| = 0.$$

I realize that I am not being complete and perhaps rigourous here. But I don't want to take the time here 
and refer you to mathmatical texts dealing with Linear Vector Spaces, Unitary Spaces, and Hilbert Spaces.\\

\underline{Example of a Hilbert Space}

Consider the set of all infinite sequencees, $\{[x_i],[x'_i],...\}$, wherer $[x_i]$ is an infinite sequence and 
$\sum_i \|x_i\|^2$ is finite. This set of sequences form a Hilbert Space.\\

\underline{2nd Example of a Hilbert Space}

Consider the set of real variables, $q_1, q_2, ..., q_k$, defined over some regions and the set of all functions
$f(q_1,q_2,...,q_k)$ such that
$$ \int\int...\int |f(q_1,q_2,...,q_k)|^2 \, dq_1 dq_2...dq_k < \infty.$$
This set of functions form a Hilbert Space where the scalar product for two of the functions, $f$ and $g$, is
defined as $$\braket{f}{g} = \int\int...\int f*g \, dq_1 dq_2...dq_k.$$

In fact, this is just the space of square-integrable functions which we use the the Schr\"odinger picture.\\

If $\braket{\phi}{\psi} = 0$ with $\phi \ne 0$ and $\psi \ne 0$, the two states described by $\phi$ and $\psi$ are said to be orthonormal.\\

Some further properties of out scalar product properties are by definition:

1. $\braket{\phi}{\psi}^* = \braket{\psi}{\phi}$\\
2. $\braket{\phi}{\lambda_1 \psi_1 + \lambda_2 \psi_2} = \lambda_1\braket{\phi}{\psi_1} + \lambda_2\braket{\phi}{\psi_2}$\\
3. $\braket{\psi}{\psi} \ge 0$ and $\braket{\psi}{psi} = 0 \iff \psi = 0$\\
4. Hermitian conjugate of an operator. \\
In general, when an operator $A$ operates on a wave function, it changes it into another wave function, e.g., $A\psi = \psi'$.
$A^\dagger$ will be defined as the ``Hermitian conjugate'' of the operator $A$ and has the property that 

$$ \braket{\phi}{A\psi} \equiv \braket{A^\dagger \phi}{\psi}$$

If $A^\dagger = A$, $A$ is said to be a Hermitian Operator. The expectation value of all Hermitian Operators are real. That is 
$$ \expect{A} \equiv \expect{\psi,A\psi} = \expect{A^\dagger\psi,\psi} = \expect{A\psi,\psi} = \expect{\psi,A\psi}^*$$
Therefore $\expect{A}$ is real. 

Using properties 1, 2, and 3 above, one can derive Schwarz's inequality which states that 

\[ \expect{\phi,\phi}\expect{\psi,\psi} \ge |\expect{\phi,\psi}|^2 \] with the equality sign holding iff $\phi = \lambda \psi$,

There are many operators in quantum mechanics, but all operators corresponding to observables, e.g, position, linear momentum, 
angular momentum, etc., are Hermitian operators. The measurement of an observable $A$ will generally give a ``spread'' or ``distribution''
of values of $A$. We define the ``uncertainty'' in $A$ as $\Delta A$ where 
\[ (\Delta A)^2 = \expect{A^2} - \expect{A}^2. \]
If $\Delta A = 0$, a restriction is placed on the expression $A\psi$ where, of course, $A$ is an Hermitian operator. The expectation 
value of $A$ is \[ \expect{A} = { \expect{\psi,A\psi}\over \expect{\psi,\psi}}\] which reduces to $\expect{\psi, A\psi}$ if $\psi$ is normalized.
If $\Delta A = 0$, we have 
\[ { \expect{\psi, A^2 \psi} \over \expect{\psi,\psi}} = { \expect{\psi,A\psi}^2\over \expect{\psi,\psi}^2} \] and as 
$\expect{\psi,A^2 \psi } = \expect{A\psi,A\psi},$ we have 
\[ \expect{\psi,\psi}\expect{A\psi,A\psi} = \expect{\psi,A\psi}^2. \] \\

This is just Schwarz's inequality with $\phi = A\psi$ and the equality sign holding. Thus $\phi = a\psi$ where $a$ is a complex constant
in general. So $A \psi = a\psi$ and then $\expect{A} = a$. But $A$ is an Hermitian operator and so $\expect{A}$ and therefore $a$ is real.\\

Using $\psi_a$ instead of $\psi$ above, we have that whenever $\Delta A = 0, A\psi_a = a \psi_a$. This is called an eigenvalue equation
with $a$ the ``eigenvalue'' of the operator $A$ and $\psi_a$ the ``eigenfunction''.\\

$e^{ {ipx / \hbar}}$ is an eigenfunction of the operator $P_x$. i.e., 

\[ P_x e^{ipx/\hbar} = {\hbar\over i} {\partial \over \partial x} e^{\i px/\hbar} = p e^{ipx/\hbar}.\]

But this eigenfunction is not square integrable and does not belong to our Hilbert space. Note that this eigenfunction had a continuous (and not
a discrete) spectrum.\\

Now it is quite easy to show that two eigenfunctions of the operator $A$ with different eigenvalues are orthogonal and linearly independent. To 
show this, consider 

\begin{eqnarray*}
A \psi_a&=&a \psi_a \quad \hbox{and} \\
A \psi_b&=&b \psi_b\\
\end{eqnarray*} 

Now
\[ \expect{\psi_b, A \psi_a} - \expect{\psi_a, A\psi_b}^* = (a - b)\expect{\psi_b,\psi_a},\]
note that $\expect{\psi_b,A\psi_b}^* = \expect{\psi_b,A\psi_a}$. Therefore
$(a-b)\expect{\psi_b,\psi_a} = 0$ and so $\expect{\psi_b,\psi_a} = 0$ if $a\ne b$. \\

To be linearly dependent we need to find non-zero $\lambda_a$ and $\lambda_b$ such that $\lambda_a \psi_a + \lambda_b \psi_b = 0$. 
Taking the scalar product of this last expression with $\psi_a$, we have 

\[ \lambda_a \expect{\psi_a,\psi_a} + \lambda_b \expect{\psi_a,\psi_b} = 0,\] or $\lambda_a \expect{\psi_a,\psi_a} = 0$. But if $\psi_a\ne 0$, we need
$\lambda_a = 0$. Likewise, $\lambda_b = 0$ and so $\psi_a$ and $\psi_b$ are linearly independent. \\

We see that ``non-degenerate'' eigenfunctions are orthogonal. If the eigenvalues are equal, it is not clear whether or not they are orthogonal. However,
they can be made orthogonal by the 	``Schmidt orthogonality process'' which goes as follows:\\

Consider the set of eigenfunctions $\psi_1, \psi_2, \psi_3,..., \psi_N$ all of which have the same eigenvalues, i.e., eigenfunctions are all equal.

1. Take $\phi_1 = c_1 \psi_1$  and pick $c_1$ so that $\phi_1$ is normalized, i.e., $c_1^2 = 1/\expect{\psi_1,\psi_1}.$\\
2. Next take $c_2 \phi_2 = \psi_2 - \phi_1\expect{\phi_1,\psi_2}$.\\
In this case, we see that $\expect{\phi_1,\phi_2} = 0$ and we can pick $c_2$ such that $\expect{\phi_2,\phi_2} = 1$.\\
3. Next take $c_3 \phi_3 = \psi_3 - \phi_1\expect{\phi_1,\psi_3} - \phi_2\expect{\phi_2,\psi_3}$. We see that $\expect{\phi_1,\phi_3} = 0$ and $\expect{\phi_2,\phi_3} = 0$
and we pick $c_3$ such that $\expect{\phi_3,\phi_3} = 1$.\\
4,..,N Just continue on in this manner until one has a set of $\phi_1, \phi_2,..., \phi_N$ of orthonormal functions where $\expect{\phi_i, \phi_j} = \delta_{ij}$.

\section{Uncertainty Principle from Schwarz's Inequality}

   Schwarz'z inequality says that fo two functions $f$ and $g$ 
   \[ (f,f) (g,g) \ge |(f.g)|^2\] with equality if and only if $f=\lambda g$. \\
   
   Let us consider two Hermition operators $A$ and $B$ from which we construct two more Hermitian operators $]\alpha = A - <A>$ and $\beta - B - <B>$. Let $f = \alpha \psi$ and $g = \beta \psi$. Then
   (\[ (f,f) = (\alpha \psi, \alpha \psi) = (\psi, \alpha^2 \psi) = <\alpha^2> = <A^2> - <A>^2 = \Delta A^2\] and likewise \[(g,g) = \Delta B^2.\] Using these in Schwarz's inequality we have
   $$ \Delta A^2 \Delta B^2 \ge | (\alpha \psi, \beta \psi)|^2 = |(\psi,\alpha\beta \psi)|^2 = {1\over 4}|(\psi,\{ \alpha, \beta\}\psi) + (\psi, \left[ \alpha,\beta\right]\psi)|^2$$  where we used 
   $$\alpha\beta = {1\over2}(\alpha\beta + \beta\alpha) + {1\over2} (\alpha\beta - \beta\alpha).$$\\
   For Hermitian operators $$(\psi,\beta \alpha \psi) = (\alpha^\dagger\beta^\dagger\psi, \psi) = (\alpha \beta \psi, \psi) = (\psi, \alpha \beta \psi )^*$$. Also because $\alpha$ and $\beta$ are Hermitian we also have the following 
   properties 
   \begin{eqnarray*}
   (\beta\alpha)^\dagger &=& \alpha^\dagger\beta^\dagger = \alpha\beta\\
   (\alpha\beta)^\dagger &=& \beta^\dagger\alpha^\dagger = \beta\alpha\\
   \end{eqnarray*}From which we also have
   \begin{eqnarray*}
   \alpha\beta + \beta\alpha &=& ~(\alpha\beta + \beta\alpha)^\dagger \mbox{ Hermitian: All Real Eigenvalues}\\ 
   \alpha\beta - \beta\alpha &=&   -(\alpha\beta - \beta\alpha)^\dagger \mbox{ Anti-Hermitian: All Imaginary Eigenvalues}
   \end{eqnarray*}Therefore
    \begin{eqnarray*}
   (\psi,(\alpha\beta + \beta\alpha)\psi) &=& (\psi,(\alpha\beta + \beta\alpha)^\dagger\psi)  = 2\Re(\psi,\alpha\beta\psi)\\ 
   (\psi,(\alpha\beta - \beta\alpha)\psi) &=& -(\psi,(\alpha\beta - \beta\alpha)^\dagger\psi)  = 2\Im(\psi,\alpha\beta\psi)
   \end{eqnarray*} 
    Therefore the above expression for $\Delta A^2\Delta b^2$ reduces to 
   $$ \Delta A^2 \Delta B^2 \ge {1\over 4} |(\psi,\{ \alpha, \beta\}\psi)|^2 + {1\over4}|(\psi, \left[ \alpha,\beta\right]\psi)|^2.$$\\
   We can always strengthen the inequality by dropping the 1st term on the right hand side. Then
   $$\Delta A^2 \Delta B^2 \ge {1\over 4} |(\psi, \left[ \alpha,\beta\right]\psi)|^2.$$\\
   Let us look at this last expression for the special case where $A=x$ and $B=p$.\\
   Since $\left[\alpha,\beta\right] = \left[A,B\right] = \left[x,p_x\right] = i\hbar$ in this case we have
   $$\Delta x\Delta p_x \ge \hbar/2$$ which is the well-known uncertainty principle. \\
   In all cases of two Hermitian operators which commute, we have with $\left[A,B\right] = 0$ 
   $$\Delta A^2 \Delta B^2 \ge {1\over4}| (\psi,\{\alpha,\beta\}\psi)|^2.$$
   Can the equality sign ``hold'' and if so will $\Delta A\Delta B = 0$? For this to happen, we need $(\psi,\{\alpha,\beta\}\psi) = 0$ and $f = \lambda g$ or $\alpha \psi = \lambda \beta \psi$. This will be the case if
   $\psi$ is  a simultaneous eigenfunction of both $A$ and $B$. For example, if $A\psi = a\psi$ and $B\psi =b\psi$, then we know that $\Delta A = \Delta B = 0$. Then $\alpha \psi = \lambda\beta\psi$ with
   $\lambda = a/b$. Furthermore, it is quite easy to show that $(\psi,\{\alpha,\beta\}\psi) = 0$. \\
   However, in general, $\psi$ need not be a simultaneous eigenfunction of $A$ and $B$ and $(\psi,\{\alpha,\beta\}\psi)$ will not in general be zero. In these cases, we will have the more general 
   relationship $\Delta A\Delta B >0$ even though $A$  and $B$ commute. \\
   Now let us return to the above one-dimensional case, $\Delta x \Delta p_x \ge \hbar/2$ and inquire what happens when the equality sign holds, i.e., when $\Delta x\Delta p_x = \hbar/2$.\\   
   For this to happen, we need $f=\lambda g$ and $(\psi\{\alpha,\beta\}\psi) = 0$. The latter condition tells us 
   \[ 0 = (\psi,(\alpha\beta + \beta\alpha)\psi) = (\psi,\alpha g) + (\psi,\beta f) = (\alpha\psi,g) + (\beta\psi,f) = (f,g) + (g,f).\]\\
   Using $f=\lambda g$, we have
   \[ (\lambda g,g) + (g,\lambda g) = (\lambda^* + \lambda) (g,g) = (\lambda^* + \lambda)\Delta p_x^2 = 0.\]
   So for the non-trivial case where $\Delta p_x^2 \ne 0$, we see that $\lambda^* + \lambda = 0$ or that $\lambda$ is pure imaginary. \\
   Let us now determine $\psi(x)$ for this case. Just to make the math easier, we study the special case where $<x> = <p> = 0$. Now $f = \lambda g$ becomes $\alpha \psi = \lambda \beta \psi$ or
   $$x\psi = {\lambda \hbar\over i} {\partial \psi\over \partial x}.$$
   Integrating gives ${\displaystyle \psi(x) = N \exp({ix^2\over 2\lambda \hbar})}$\\
   We see that for $\psi(x)$ to be zero at $x=\pm \infty$, we need $\lambda$ to be a negative imaginary number. We already knew it was imaginary.\\
   For convenience, we define $\lambda = \displaystyle {-i\over \nu^2 \hbar}$, where $\nu^2$ is a real positive number. Then $\displaystyle \psi = N\exp({-\nu^2 x^2\over 2})$. \\
   Now  $$(\psi,\psi) = 1 = |N|^2 \int\, e^{-\nu x^2}\, dx = {N^2\sqrt{\pi}\over \nu}$$ and 
   $$\Delta x^2 = (\psi, x^2\psi) = |N|^2 \int\, x^2 e^{\nu x^2}\, dx = {|N|^2 \sqrt{\pi}\over 2\nu^3}.$$ 
   
   