\section{The Dirac Delta Function}
Consider the relation $\displaystyle y_i = \sum_j \, a_{ij} x_j$ rewritten as $\displaystyle y(i) = \sum_j a(i,j) x(j)$ which states that $y(i)$ is a linear combination of the $x(i)'s$.
If thse indices were continuous, we would have an expression of the form
$$f(x) = \int\, G(x,x') \, g(x') \, dx'.$$ One says that $f(x)$ is a linear functional of $g(x)$ and $G(x,x')$ is called the ``kernel'' which depends in general on both $x$ and $x'$. We see that it is linear
because if $f_1 = \int\, G g_1\,dx$ and $f_2 = \int\, G g_2\,dx$, then $f_1 + f_2 = \int\, G(g_1 + g_2)\, dx$. A good example of this is our Fourier transform (in one dimension)
$$\psi(x) = {1\over \sqrt{2\pi}}\int\, e^{ikx}i, \phi(k)\, dk.$$ Here $\psi(x)$ is a linear functional of $\phi(k)$ with a kernel of $\displaystyle {1\over\sqrt{2\pi}}e^{-ikx}$. \\
On the other hand, we could have considered $\displaystyle y(i) = \sum_j\, a(i,j) x(j)$ as a linear transformation. Similarly, 
$$f(x) = \int\, G(x,x') g(x')\,dx$$ could be considered as a linear transformation where $g(x)$ are vectors with a set of continuous indices and $G(x,x')$ being the matrix for the transformation.\\
In the discrete indices case, the ``identity'' transformation is given by $a(i,j) = \delta_{ij}$. But in the continuous indices case, there is no function of $x$ and $x'$ which does a similar thing. However, 
this doesn't bother physicists who define a function called the ``Dirac delta function'', $\delta(x-x')$, which replaces $G(x,x')$ such that $$f(x) = \int \, \delta(x-x') f(x')\, dx'.$$ 

\subsection{Common Representations of the $\delta$-function}
Some of the more common forms 
of representation for the Dirac delta function are:
\begin{eqnarray*}
1.\ \delta(x) &=& \lim_{\epsilon\rightarrow 0} {\epsilon\over \pi (\epsilon^2 + x^2)}\\
2.\ \delta(x) &=& \lim_{\epsilon\rightarrow 0} {1\over {dg\over dx} } {1\over \epsilon(b-a)}
\end{eqnarray*}where $g(x)$ is any smooth monotonic function in $(a,b)$ with $g(a) = -\infty$ and $g(b) = +\infty$. 
\begin{eqnarray*}
3.\ \delta(x) &=& \lim_{N\rightarrow \infty} {\sin(Nx)\over \pi x}\\
4.\ \delta(x) &=& {1\over 2\pi} \int_{-\infty}^{+\infty} \, e^{\pm i\mu x}\, du\\
5.\ \delta(x) &=& \lim_{\epsilon\rightarrow 0} {e^{-x^2/\epsilon}\over \sqrt{\pi\epsilon}}\\
6.\ \delta(x) &=& \lim_{\epsilon\rightarrow 0} {\theta(x+\epsilon) - \theta(x-\epsilon)\over 2\epsilon} \rightarrow {d\theta(x)\over dx}\\
\end{eqnarray*}where 
\begin{eqnarray*}
\theta(x) &=& 0 \mbox{ for } x<0\\
\theta(x) &=& 1 \mbox{ for } x > 0.
\end{eqnarray*}
Let us look at the firs one, i.e., 
$\delta(x-x') = \lim_{\epsilon\rightarrow 0} D(x-x',\epsilon)$, where $\displaystyle D(x,\epsilon) = {\epsilon\over \pi (\epsilon^2 + x^2)}.$
$D(x,\epsilon)$ for three values of $\epsilon$ are shown in the Figure at the right. We see that as $\epsilon$ becomes smaller, $D$ becomes more peaked around $x=0$ and is 
``practically'' zero elsewhere. Now $\displaystyle \int_{-\infty}^{+\infty} \, D(x,\epsilon)\, dx = 1$ and is independent of $\epsilon$. \\
Now if $f(x)$ is continuous around $x\approx x'$, then for small $ \epsilon$ (sharply peaked around 0), we have
$$ \int_{-\infty}^{+\infty}\, f(x') D(x-x')\,dx' \approx f(x) \int\, D(x-x',\epsilon)\, dx' = f(x).$$
The above is not a proof. However, if $f(x)$ is bounded everywhere, one can show that $$lim_{\epsilon\rightarrow 0}\int_{-\infty}^{+\infty}\, f(x) D(x,\epsilon)\, dx = f(0).$$
This is just what we want the $\delta$ function to do and if it does it, the exact form of $(D(x,\epsilon)$ is not important.\\
We will assume that a $\delta$ function exists such that 
$$\int_{-\infty}^{+\infty}\, f(x') \delta(x-x')\, dx' = f(x).$$
Furthermore, we should observer that the $\delta$ function will only have meaning when it appears under the integral sign.\\
Later on in this section, we will make a brief digression into Riemann-Stieltjes integrals where we shall see that integrals like the above can be handled with 
more mathematical rigor. \\

\subsection{3-dimensional $\delta$-functions}
Extension into 3 dimensions is ``straight forward''. 
$$\delta(\vec{r} - \vec{r'}) = \delta(x-x')\delta(y-y')\delta(z-z')$$ so that 
$$\int\, f(\vec{r'}) \delta(\vec{r} - \vec{r'}) d\vec{r'} = \int\, f(x',y',z') \delta(x-x')\delta(y-y')\delta(z-z')\, dx'dy'dz' = f(x,y,z) = f(\vec{r}).$$
From our work with Fourier transforms
\begin{eqnarray*}
\delta(\vec{r}-\vec{r'}) &=& {1\over (2\pi)^3}   \int_{-\infty}^{+\infty}\, e^{\pm i \vec{k}\cdot(\vec{r}-\vec{r'}) }\, d\vec{k}\\
\delta(\vec{p}-\vec{p'}) &=& {1\over (2\pi)^3} \int_{-\infty}^{+\infty}\, e^{\pm i \vec{r}\cdot(\vec{p}-\vec{p'}) }\, d\vec{r}.
\end{eqnarray*}
In spherical coordinates $(r.\theta, \phi)$,
$$\delta(\vec{r}-\vec{r'}) = {\delta(r - r')\over r^2} \sum_{l,m}\, Y_{lm}^*(\theta.\phi)Y_{lm}(\theta'.\phi'),$$ where the $Y_{lm}(\theta,\phi)$'s are the spherical harmonics.\\
Example: Let us use the 3rd representation above and show that 
\begin{eqnarray*}
\int_A^B\, f(x)\delta(x)\, dx &=& f(0) \mbox{ if } A \le 0 \le B\\
                                          &=& 0 \mbox{ otherwise.}
\end{eqnarray*} We have 
$$\int_A^B\, f(x) \lim_{N\rightarrow \infty} {\sin(Nx)\over \pi x}\, dx.$$ We not for $x$ not near zero and $N$ large, the $\sin(Nx)$ oscillates rapidly. Consequently we wouldn't expect a large contribution to the integral
from these regions as long as $f(x)$ is a ``smooth'' and ``reasonably well behaved'' function.\\
As $\displaystyle \lim_{x\rightarrow0} {\sin(Nx)\over x} = N$, we will have around $x=0$ just $\displaystyle \int_{x\approx0} \, N f(x)\, dx$. Thus our ``main'' contributions should come around
$x\approx0$ and it shouldn't be too surprising to get just $f(0)$.\\
Let us show this in detal. We let 
$$I = {1\over\pi}\lim_{N\rightarrow\infty} \int_A^B\, f(x) {\sin(Nx)\over x}\,dx.$$
Integrate by parts with $u=f(x)$ and $\displaystyle dv = {\sin(Nx)\over x}\,dx$.
$$I = {1\over\pi} \lim_{N\rightarrow\infty} \left\{\left[ f(x) \int_A^x\, {\sin(Ny)\over y} \, dy \right]_A^B  - \int_A^B f'(x)\,dx \int_A^x\, {\sin(Ny)\over y} \,dy \right\}.$$
WIth $z = Ny$, 
$$I = {1\over\pi}\lim_{N\rightarrow\infty} \left\{ \left[ f(x) \int_{NA}^{Nx}\, {\sin(Ny)\over y}\,dy\right]_A^B - \int_A^B f'(x)\,dx \int_{NA}^{Nx}\, {\sin(Ny)\over y}\,dy \right\}.$$
First, we look at the 1st term on the right hand side. For the lower limit, $Nx\rightarrow NA$ and the integral is zero. For the upper limit $Nx \rightarrow NB$. If $A$ and $B$ are both 
$\left\{\begin{matrix}
\mbox{positive} \\
\mbox{negative} 
\end{matrix}\right\}$, the integral is still zero as $\displaystyle \lim_{N=\infty} NA = \lim_{N \rightarrow \infty} NB = \{\pm \infty\}$.
If $A$ is negative and $B$ us positive, 
$$\lim_{N\rightarrow\infty}\int_{NA}^{NB}\, {\sin(z)\over z}\,dz = \int_{-\infty}^{+\infty} {\sin(z)\over z}\,dz = \pi.$$
So the 1st term $= f(B)$ if $A<0$ and $B>0$ and is zero otherwise.\\
Similar reasoning applies to the 2nd term. It will be zero unless $A$ is negative and $x$ is positive in the integral over $dy$. This means that in the integral over $dx$, we can ``neglect'' that
part it where $x<0$. So we replace $\int_A^B\,dx$ by $\int_0^B\, dx$. With this, the 2nd term becomes 
$$- {1\over\pi} \int_A^B f'(x)\,dx \left[ \lim_{N\rightarrow\infty} \int_{NA}^{Nx} \, {\sin(Ny)\over y} \,dy \right].$$
The bracket gives $\pi$ for all $x>0$ and the second term becomes $-f(B) + f(0)$. So 
\begin{eqnarray*}
I = \int_A^B\, f(x)\delta(x)\, dx &=& f(0) \mbox{ if } A<0 \mbox{ and } B>0\\
&=& 0 \mbox{ otherwise}
\end{eqnarray*}
End example.\\

\subsection{Relations involving the $\delta$-Function}
Some of the more ``useful'' relationships involving the $\delta$ function are: \hbox{(see Schiff pp 57)}
\begin{eqnarray*}
&1.~& \int\, \delta(x)\, dx  = 1\\
&2.~& \int\, \delta(-x)\, dx = \delta(x) \\
&\null&~~~~~~~~~i.e. \int_{-\infty}^{+\infty}\, f(x)\delta(x) \, dx = \int_{-\infty}^{+\infty}\, f(x)\delta(-x) \, dx = f(0)\\
&3.~& \delta(ax) = {1\over a} \delta(x) \mbox{ for } a> 0\\
&4.~& x\delta(x) = 0\\
&\null&~~~~~~~~~ i.e. \int_{-\infty}^{+\infty}\, f(x) x \delta(x) \, dx = f(x) x \big|_{x=0} = 0 \\
&5.~& \int\,\delta(x-x'')\delta(x''- x')\, dx'' = \delta(x-x')\\
&6.~& f(x)\delta(x-x') = f(x')\delta(x-x')\\
&7.~& \delta(x^2 - a^2) = {1\over2a} \left[\delta(x-a) + \delta(x+a)\right]\\
&8.~& x\delta'(x) = x{d\over dx}\delta(x) = -\delta(x)\\
&9.~& \delta^{(m)}(x) = (-1)^m \delta^{(m)}(-x)\\
&10.& \int\, \delta^{(m)}(x-y) \delta^{(n)}(y-a)\, dy = \delta^{(m+n)}(x-a)\\
&11.& x^{m+1}\delta^{(m)}(x) = 0\\
&12.& \int\, f(x) \delta^{(m)}(x)\, dx = (-1)^m f^{(m)}(0)\\
&\null&~~~~~~~~~~\mbox{ providing that } f^{(m)}(0) \mbox{ exists}.
\end{eqnarray*}
Now let us digress briefly and look at Riemann-Stieltjes Integrals.
We shall see that the Dirac $\delta$ function can be formulated in a ``rigorous'' manner in this case. 

\subsection{Digression on Riemann-Steltjes Integrals}
(cf. Chapter 9 of Mathematical Analysis by Apostol.)\\
\underline{Riemann Integrals}\\
Many physicists and scientist never see nor never use any integral besides the Riemann integral.\\
For a simplified picture of this integral, let us divide the interval $[a,b]$ up into $n$ parts, let $t_k$ be a point between $x_{k-1}$ and $x_k$, and let $\Delta x_k = x_k - x_{k-1}$. Then we normally think of 
the Riemann integral as 
$$\int_a^b\, f(x)\, dx = \lim_{n\rightarrow\infty} \sum_{k=1}^n\, f(t_k) \Delta x_k$$ where we have assumed that the limit exists.\\
Graphically, this integral represents the area under the curve $f(x)$. Note that $f(x)$ must be continuous but that it does not need to have a continuous first derivative for 
the integral to exist. \\
Let us now look at the Riemann-Stieltjes integral which will reduce to the Riemann integral in special cases. 
\subsection{Riemann-Steltjes Integrals}
First we make a few definitions and state some theorems without proof. I refer you to Apostol for details.\\
We will be considering two functions, $f(x)$ and $g(x)$ where both may or may not be continuous or even differentiable functions. 

\begin{definition} 
$P$ is called a partition of the interval $[a,b]$ and consists of the finite set of points \\  $a=x_0<x_1<x_2<\hdots<x_{n-1}<x_n = b$.
\end{definition}

\begin{definition}
The partition $P'$ of $[a,b]$ is ``finer'' than $P$ or a ``refinement'' of $P$ if $P$ is contained in $P'$. i.e., $P\subset P'$.
\end{definition}

\begin{definition}
Let $P= \{ x_o, x_1, x_2, \hdots x_n\}$ be a partition of $[a,b]$, $\Delta \alpha_k \equiv \alpha(x_k) - \alpha(x_{k-1})$, and $t_k$ be a point in $[x_{k-1}, x_k]$. The sum
$$S(P,f,\alpha) = \sum_{k=1}^n\, f(t_k) \Delta \alpha_k$$ is called a ``Riemann-Stieltjes Sum'' of $f$ with respect to $\alpha$ on $[a,b]$.
\end{definition}
Now we say that $f$ is Riemann integrable with respect to $\alpha$ on $[a,b]$, which we denote by `` $f\in R(\alpha)$ on $[a,b]$'', if there exists a number $A$ such that:\\
For every $\epsilon>0$, there exists a partition $P_\epsilon$ of $[a,b]$ such that for every partition $P$ finer than $P_\epsilon$ and for every $t_k$ in $[x_{k-1}, x_k]$ we have
$$\left| S(P,f,\alpha) - A\right| < \epsilon.$$
When this is the case, we denote $A$ by the integral $ \displaystyle \int_a^b\, f(x)\, d\alpha(x)$ or simply bu $\displaystyle \int_a^b \, f\, d\alpha$.\\
One can now show:\\
If $f\in R(\alpha)$ and $g\in R(\alpha)$, then for arbitrary constants $c_1$ and $c_2$, we have 
$$\int_a^b\, (c_1 f + c_2 g)\, d\alpha = c_1\int_a^b\, f\, d\alpha + c_2 \int_a^b\, g\, d\alpha.$$
Similarly, if $f\in R(\alpha)$ and $f\in R(\beta)$, then 
$$\int_a^b\, f\, d(c_1 f + c_2 g)  = c_1\int_a^b\, f\, d\alpha + c_2 \int_a^b\, f\, d\beta.$$ 
So the integral is ``linear'' in both $f$ and $\alpha$.\\
If $c \in [a,b]$ and $f\in R(\alpha)$, then 
$$\int_a^b\, f\, d\alpha = \int_a^c\, f\, d\alpha + \int_c^b\, f\, d\alpha.$$

\begin{definition}
If $a<b$, we define $\displaystyle \int_b^a \, f\, d\alpha = - \int_a^b\, f\, d\alpha$ whenever $\displaystyle \int_a^b\, f\, d\alpha$ exists. Also we define
$\displaystyle \int_a^b\, f\, d\alpha = 0$. 
\end{definition}

\begin{theorem}
If $f\in R(alpha)$ on $[a,b]$, then $\alpha \in R(f)$ on $[a,b]$ and 
$$\int_a^b \ f(x) \, d\alpha(x) + \int_a^b\, \alpha(x) \, df(x) = f(b)\alpha(b) - f(a)\alpha(a).$$
\end{theorem}

This ``formula'' or ``relationship'' is known as ``the formula for integration by parts'' and tells us that if $\displaystyle \int_a^b\, f\, d\alpha$ exists, then so does
$\displaystyle \int_a^b\, \alpha\, df$.\\
Note as we go along that everything we say also applies to the Riemann integral. However, these relationships are more powerful as we shall see shortly. In fact, 
$f$ and $\alpha$ need not be continuous and/or differentiable for the above to hold.

\begin{theorem}
The Riemann Integral.\\
If $f\in R(\alpha)$ on $[a,b]$ and $\alpha(x)$ has a continuous derivative on $[a,b]$, then $d\alpha(x)$ can be replaced by $\alpha'(x)\, dx$. Also, the Riemann integral,
$\displaystyle \int_a^b\, f(x)\alpha'(x)\, dx$, exists.
We could define $\displaystyle g(x) \equiv f(x)\alpha'(x)$ and then 
$$\int_a^b\, f(x)\, d\alpha(x) = \int_a^b\, f(x)\alpha'(x)\, dx = \int_a^b\, g(x)\, dx$$ and the last expression looks more like the integrals we are accustomed to using. 
\end{theorem}

Back to our Riemann-Stieltjes integral. We note that if $\alpha(x)$ is a constant throughout $[a,b]$, then all $\alpha_k = 0$ and $\displaystyle \int_a^b \, f\, d\alpha$ exists and is zero.\\
However, out next theorem will state that if $\alpha(x)$ is constant everywhere except for a jump discontinuity at one point, then $\displaystyle \int_a^b\, f\, d\alpha$ need not exist, but if it does, 
it need not be zero.

\begin{theorem}
Given $a < c < b$ and $\alpha$ defined on $[a,b]$ by 
$\left\{ \begin{matrix}
\alpha(x) = \alpha(a) \mbox{ for } a\le x < c \\
\alpha(x) = \alpha(a) \mbox{ for } c < x \le b
\end{matrix}\right\}$
Let $f(x)$ and $\alpha(x)$ be defined on $[a,b]$ such that at least one of them is continuous from the left at $c$ and at least one of them is continuous from the right at $c$. Then 
$f\in R(\alpha)$ on $[a,b]$ and we have
$$\int_a^b \, f\, d\alpha = f(c)\left[ \alpha(c^+) - \alpha(c^-)\right].$$
\end{theorem}
This theorem tells us that we can do integrals for functions $f$ and $\alpha$ like those shown at the right. This is something we couldn't do with Riemann integrals. \\
We can have several ``combinations''.
\begin{description}
\item[] 1. $f(x)$ can be both continuous both from the right and from the left and $\alpha(x)$ discontinuous both from the right and the left.
\item[] 2. $f(x)$ is continuous from the left and discontinuous from the right. Then $\alpha(x)$ must be continuous from the right and discontinuous from the left.
\item[] 3. $f(x)$ is continuous from the right and discontinuous from the left. Then $\alpha(x)$ must be continuous from the left and discontinuous from the right.
\item[] 4. If $\alpha(x)$ is continuous from both the right and the left, then $\alpha(c^+) - \alpha(c^-) = 0$ and the integral is zero irrespective of the $f(x)$ does.
\item[] 5. One can show that if both $f$ and $\alpha$ are discontinuous from the right or from the left, then the integral does not exist.
\end{description}
Now let use use these results to ``formulate'' the Dirac $\delta$-function using Riemann-Stieltjes integrals. Let $\alpha(x)$ be the step function $\Theta(x-c)$ where
$\left\{ \begin{matrix}
\Theta(x) = 0 \mbox{ for }  0 < x\\
\Theta(x) = `\mbox{ for }    x > 0
\end{matrix}\right\}$
Then for a continuous $f(x)$, we have 
$$\int_a^b\, f(x)\, d\alpha(x) = \int_a^b\, f(x) \, d\Theta(x-c) = f(c).$$
In this case, $d\alpha(x) = d\Theta(x-c)$ behaves just like $\delta(x-c)\, dx$.\\
So if we would use Riemann-Stieltjes integrals with $\alpha(x)$ being the step function, we would never have
to introduce the Dirac $\delta$-function. However, physicists don't do this. They use the Dirac $\delta$-function and Riemann integrals and usually don't run into troubles as 
long as the functions $f(x)$ are ``well behaved'' and ``so forth''.//
But if one should get into difficulties, he would have to ``back track'' and do a more rigorous treatment. \\
Mathematicians don't stop here. They go further and talk about Lebesgue integrals where they can use functions which are not only discontinuous but which 
may only be defined in certain regions and at certain points. Then one must become involved with Lesbesgue measure theory.